%%writefile customer_segmentation.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation Analysis using RFM and K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning\n",
    "\n",
    "Key steps:\n",
    "- Load transaction data from CSV\n",
    "- Remove records with missing CustomerID\n",
    "- Convert invoice dates to datetime format\n",
    "- Create temporal features (day/week/month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (update path as needed)\n",
    "file_path = \"/content/df.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data cleaning\n",
    "df_cleaned = df.dropna(subset=['CustomerID'])\n",
    "df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
    "\n",
    "# Create temporal features\n",
    "df_cleaned['InvoiceDay'] = df_cleaned['InvoiceDate'].dt.date\n",
    "df_cleaned['InvoiceWeek'] = df_cleaned['InvoiceDate'].dt.to_period('W').astype(str)\n",
    "df_cleaned['InvoiceMonth'] = df_cleaned['InvoiceDate'].dt.to_period('M').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering: RFM+ Analysis\n",
    "\n",
    "**Core RFM Features:**\n",
    "- Recency: Days since last purchase\n",
    "- Frequency: Number of transactions\n",
    "- Monetary Value: Total spending\n",
    "\n",
    "**Enhanced Features:**\n",
    "- Average order value\n",
    "- Purchase frequency variance\n",
    "- Normalized time between orders\n",
    "- Spending patterns across time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reference date\n",
    "reference_date = df_cleaned['InvoiceDate'].max()\n",
    "\n",
    "# Aggregate customer features\n",
    "customer_features = df_cleaned.groupby('CustomerID').agg(\n",
    "    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),\n",
    "    Frequency=('InvoiceNo', 'nunique'),\n",
    "    MonetaryValue=('UnitPrice', lambda x: (x * df_cleaned.loc[x.index, 'Quantity']).sum()),\n",
    "    AvgOrderValue=('UnitPrice', 'mean'),\n",
    "    AvgQuantityPerOrder=('Quantity', 'mean'),\n",
    "    DailyFrequency=('InvoiceDay', 'nunique'),\n",
    "    WeeklyFrequency=('InvoiceWeek', 'nunique'),\n",
    "    MonthlyFrequency=('InvoiceMonth', 'nunique'),\n",
    "    FrequencyVariance=('InvoiceDay', lambda x: np.var(x.value_counts(), ddof=1))\n",
    ").reset_index()\n",
    "\n",
    "# Handle missing values\n",
    "customer_features.fillna({'FrequencyVariance': 0}, inplace=True)\n",
    "\n",
    "# Create enhanced features\n",
    "customer_features['Recency'] /= customer_features['Recency'].max()\n",
    "customer_features['SpendingPerOrder'] = customer_features['MonetaryValue'] / (customer_features['Frequency'] + 1e-5)\n",
    "customer_features['TimeBetweenOrders'] = customer_features['Recency'] / (customer_features['Frequency'] + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling\n",
    "\n",
    "**Why MinMax Scaling?**\n",
    "- Preserves original distribution shape\n",
    "- Brings all features to 0-1 range\n",
    "- Better suited for frequency-based metrics than standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale features\n",
    "features_scaled = scaler.fit_transform(customer_features.drop(columns=['CustomerID']))\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=customer_features.drop(columns=['CustomerID']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimal Cluster Determination\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "1. Elbow Method (WCSS)\n",
    "2. Silhouette Score (Higher better)\n",
    "3. Davies-Bouldin Index (Lower better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics storage\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "cluster_range = range(2, 11)\n",
    "\n",
    "# Calculate metrics for different k values\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(features_scaled_df)\n",
    "    \n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(features_scaled_df, kmeans.labels_))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(features_scaled_df, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualization of Cluster Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow Method Plot\n",
    "ax[0].plot(cluster_range, wcss, marker='o', linestyle='-')\n",
    "ax[0].set_title('Elbow Method')\n",
    "ax[0].set_xlabel('Number of Clusters')\n",
    "ax[0].set_ylabel('WCSS')\n",
    "\n",
    "# Silhouette Score Plot\n",
    "ax[1].plot(cluster_range, silhouette_scores, marker='s', linestyle='-')\n",
    "ax[1].set_title('Silhouette Scores')\n",
    "ax[1].set_xlabel('Number of Clusters')\n",
    "ax[1].set_ylabel('Score')\n",
    "\n",
    "# Davies-Bouldin Plot\n",
    "ax[2].plot(cluster_range, davies_bouldin_scores, marker='d', linestyle='-')\n",
    "ax[2].set_title('Davies-Bouldin Index')\n",
    "ax[2].set_xlabel('Number of Clusters')\n",
    "ax[2].set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Clustering with K=4\n",
    "\n",
    "**Rationale for Cluster Selection:**\n",
    "- Consensus from multiple metrics\n",
    "- Practical business interpretability\n",
    "- Cluster size balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with selected K\n",
    "best_k = 4\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "customer_features['Cluster'] = kmeans.fit_predict(features_scaled_df)\n",
    "\n",
    "# Display cluster characteristics\n",
    "cluster_summary = customer_features.groupby('Cluster').mean()\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cluster Interpretation and Business Insights\n",
    "\n",
    "**Suggested Cluster Profiles:**\n",
    "1. **High-Value Loyalists** - High frequency, high spending, low recency\n",
    "2. **At-Risk Customers** - High historical value but recent inactivity\n",
    "3. **Budget Shoppers** - Frequent purchases but low order values\n",
    "4. **Seasonal Shoppers** - Irregular purchase patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "customer_features.to_csv(\"customer_clusters.csv\", index=False)\n",
    "cluster_summary.to_csv(\"cluster_summary.csv\")\n",
    "\n",
    "print(\"Clustering results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
